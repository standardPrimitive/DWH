{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2be21b72-00bd-4529-908b-3b682cd252c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum as spark_sum, expr, datediff, when, first, max as spark_max\n",
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e0c1542b-ad0f-4e32-9420-b9f8c7c82a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация сессии Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DWH Data Pipeline - Craftsman Marketplace\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres_container:5432/postgres\" # JDBC-URL для Postgres\n",
    "jdbc_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9efb2317-07d2-4088-a2b4-84b2ec539fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========Чтение исходных таблиц-источников==========#\n",
    "# Читаем первую исходную таблицу (source1.craft_market_wide) из PostgreSQL\n",
    "df_source1 = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source1.craft_market_wide\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Читаем таблицы из source2\n",
    "df_source2_masters_products = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source2.craft_market_masters_products\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "df_source2_orders_customers = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source2.craft_market_orders_customers\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "# Соединяем две таблицы source2 по полю craftsman_id\n",
    "df_source2 = df_source2_orders_customers.join(\n",
    "    df_source2_masters_products,\n",
    "    on=\"craftsman_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Читаем таблицы из source3\n",
    "df_source3_craftsmans = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source3.craft_market_craftsmans\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "df_source3_customers = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source3.craft_market_customers\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "df_source3_orders = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"source3.craft_market_orders\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "# Объединяем три таблицы source3 по полям craftsman_id и customer_id\n",
    "df_source3 = df_source3_orders \\\n",
    "    .join(df_source3_craftsmans, on=\"craftsman_id\", how=\"inner\") \\\n",
    "    .join(df_source3_customers, on=\"customer_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a06a99ff-3f73-4221-ae26-51400b959a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========объединение данных из трех источников==========#\n",
    "# Список колонок, которые должны быть общими для всех источников\n",
    "common_columns = [\n",
    "    'craftsman_name', 'craftsman_address', 'craftsman_birthday', 'craftsman_email',\n",
    "    'product_name', 'product_description', 'product_type', 'product_price',\n",
    "    'order_created_date', 'order_completion_date', 'order_status',\n",
    "    'customer_name', 'customer_address', 'customer_birthday', 'customer_email'\n",
    "]\n",
    "\n",
    "# Собираем все DataFrame источников в список\n",
    "list_dfs = [df_source1, df_source2, df_source3]\n",
    "# Создаём пустой список для выравненных (aligned) DataFrame\n",
    "list_dfs_aligned = []\n",
    "for df_item in list_dfs:\n",
    "    # Проверяем каждую колонку из common_columns:\n",
    "    # если её нет в df_item, используем lit(None) вместо неё\n",
    "    new_df = df_item.select([\n",
    "        col(c) if c in df_item.columns else lit(None).alias(c)\n",
    "        for c in common_columns\n",
    "    ])\n",
    "    # Добавляем готовый aligned_df в список list_dfs_aligned\n",
    "    list_dfs_aligned.append(new_df)\n",
    "\n",
    "# Объединяем (union) все три приведённых DataFrame в один df_unified\n",
    "df_unified = list_dfs_aligned[0].union(list_dfs_aligned[1]).union(list_dfs_aligned[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "096be995-2a92-4c76-a4db-97720aaa5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final unified DataFrame count: 2997\n",
      "Dim Craftsman count: 2995\n",
      "Dim Customers count: 2997\n",
      "Dim Products count: 2990\n",
      "Fact Orders count: 2997\n"
     ]
    }
   ],
   "source": [
    "# ========== 3. Формирование таблиц измерений (d_*) и таблицы фактов (f_orders) ==========\n",
    "\n",
    "# Создаём окно для ранжирования мастеров (d_craftsmans)\n",
    "w_craftsman = Window.orderBy(\"craftsman_name\", \"craftsman_birthday\")\n",
    "# Формируем таблицу измерения мастеров, уникализируем по (craftsman_name, craftsman_birthday)\n",
    "df_dim_craftsmans = df_unified.select(\n",
    "    \"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"\n",
    ").dropDuplicates([\"craftsman_name\", \"craftsman_birthday\"]) \\\n",
    " .withColumn(\"craftsman_id\", row_number().over(w_craftsman))\n",
    "\n",
    "# Аналогичное окно для клиентов\n",
    "w_customer = Window.orderBy(\"customer_name\", \"customer_birthday\")\n",
    "df_dim_customers = df_unified.select(\n",
    "    \"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"\n",
    ").dropDuplicates([\"customer_name\", \"customer_birthday\"]) \\\n",
    " .withColumn(\"customer_id\", row_number().over(w_customer))\n",
    "\n",
    "# Аналогичное окно для продуктов\n",
    "w_product = Window.orderBy(\"product_name\", \"product_price\")\n",
    "df_dim_products = df_unified.select(\n",
    "    \"product_name\", \"product_description\", \"product_type\", \"product_price\"\n",
    ").dropDuplicates([\"product_name\", \"product_price\"]) \\\n",
    " .withColumn(\"product_id\", row_number().over(w_product))\n",
    "\n",
    "# Факт: Заказы (f_orders)\n",
    "# Создаём окно для фактов (f_orders), будем генерировать order_id\n",
    "w_fact_orders = Window.orderBy(\"product_id\", \"craftsman_id\", \"customer_id\")\n",
    "# Формируем таблицу фактов, связываем по ключам\n",
    "df_fact_orders = df_unified \\\n",
    "    .join(df_dim_products, [\"product_name\", \"product_price\"], \"left\") \\\n",
    "    .join(df_dim_craftsmans, [\"craftsman_name\", \"craftsman_birthday\"], \"left\") \\\n",
    "    .join(df_dim_customers, [\"customer_name\", \"customer_birthday\"], \"left\") \\\n",
    "    .select(\n",
    "        row_number().over(w_fact_orders).alias(\"order_id\"), # Генерируем surrogate key для заказов\n",
    "        col(\"product_id\"),\n",
    "        col(\"craftsman_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"order_created_date\"),\n",
    "        col(\"order_completion_date\"),\n",
    "        col(\"order_status\")\n",
    "    )\n",
    "\n",
    "# Добавляем столбец загрузки (load_dttm)\n",
    "load_timestamp = current_timestamp()\n",
    "df_dim_craftsmans = df_dim_craftsmans.withColumn(\"load_dttm\", load_timestamp)\n",
    "df_dim_customers = df_dim_customers.withColumn(\"load_dttm\", load_timestamp)\n",
    "df_dim_products = df_dim_products.withColumn(\"load_dttm\", load_timestamp)\n",
    "df_fact_orders = df_fact_orders.withColumn(\"load_dttm\", load_timestamp)\n",
    "\n",
    "# Печатаем размеры (кол-во строк) для контроля\n",
    "print(\"Final unified DataFrame count:\", df_unified.count())\n",
    "print(\"Dim Craftsman count:\", df_dim_craftsmans.count())\n",
    "print(\"Dim Customers count:\", df_dim_customers.count())\n",
    "print(\"Dim Products count:\", df_dim_products.count())\n",
    "print(\"Fact Orders count:\", df_fact_orders.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "59d2adf0-7d76-4fb8-ac59-cb6ac88c6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========функция записи уникальных данных в DWH (чтобы не дублировать записи) ==========#\n",
    " \n",
    "#  Читает значения существующих ключей из таблицы DWH, \n",
    "#   исключает дубликаты, после чего дозаписывает (append) в целевую таблицу.\n",
    "def write_unique_data(df_data, table_name, key_col, jdbc_url, jdbc_props):\n",
    "\n",
    "    # Читаем уже существующие ключи\n",
    "    existing_key_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=jdbc_props)\n",
    "    existing_keys = existing_key_df.select(key_col).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Фильтруем новые записи по тому, что ключ не входит в уже имеющиеся\n",
    "    df_filtered = df_data.filter(~df_data[key_col].isin(existing_keys))\n",
    "\n",
    "    # Запись только уникальных значений\n",
    "    df_filtered.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=jdbc_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b63d6eae-39b0-46aa-a085-a872c67d8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========запись в DWH: измерения и факт ==========\n",
    "\n",
    "write_unique_data(\n",
    "    df_data=df_dim_craftsmans,\n",
    "    table_name=\"dwh.d_craftsmans\",\n",
    "    key_col=\"craftsman_id\",\n",
    "    jdbc_url=jdbc_url,\n",
    "    jdbc_props=jdbc_props\n",
    ")\n",
    "\n",
    "write_unique_data(\n",
    "    df_data=df_dim_customers,\n",
    "    table_name=\"dwh.d_customers\",\n",
    "    key_col=\"customer_id\",\n",
    "    jdbc_url=jdbc_url,\n",
    "    jdbc_props=jdbc_props\n",
    ")\n",
    "\n",
    "write_unique_data(\n",
    "    df_data=df_dim_products,\n",
    "    table_name=\"dwh.d_products\",\n",
    "    key_col=\"product_id\",\n",
    "    jdbc_url=jdbc_url,\n",
    "    jdbc_props=jdbc_props\n",
    ")\n",
    "\n",
    "write_unique_data(\n",
    "    df_data=df_fact_orders,\n",
    "    table_name=\"dwh.f_orders\",\n",
    "    key_col=\"order_id\",\n",
    "    jdbc_url=jdbc_url,\n",
    "    jdbc_props=jdbc_props\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2dbf3c3f-86a6-42bb-abe1-93a57b6020b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# ==========Организация инкрементальной загрузки в витрину dwh.craftsman_report_datamart ==========#\n",
    "\n",
    "# Читаем таблицы из DWH (уже заполненные измерения и факт)\n",
    "df_d_craftsmans = spark.read.jdbc(jdbc_url, \"dwh.d_craftsmans\", properties=jdbc_props)\n",
    "df_d_customers = spark.read.jdbc(jdbc_url, \"dwh.d_customers\", properties=jdbc_props)\n",
    "df_d_products = spark.read.jdbc(jdbc_url, \"dwh.d_products\", properties=jdbc_props)\n",
    "df_f_orders = spark.read.jdbc(jdbc_url, \"dwh.f_orders\", properties=jdbc_props)\n",
    "\n",
    "# Читаем таблицу с датами загрузок\n",
    "df_load_dates = spark.read.jdbc(jdbc_url, \"dwh.load_dates_craftsman_report_datamart\", properties=jdbc_props)\n",
    "\n",
    "# Получаем последнюю дату загрузки\n",
    "max_date_row = df_load_dates.agg(spark_max(\"load_dttm\")).collect()[0][0]\n",
    "last_load_date = max_date_row if max_date_row is not None else None\n",
    "\n",
    "# Фильтрация измерений и фактов по load_dttm (если last_load_date не None)\n",
    "if last_load_date:\n",
    "    df_d_craftsmans = df_d_craftsmans.filter(col(\"load_dttm\") > last_load_date)\n",
    "    df_d_customers = df_d_customers.filter(col(\"load_dttm\") > last_load_date)\n",
    "    df_d_products = df_d_products.filter(col(\"load_dttm\") > last_load_date)\n",
    "    df_f_orders = df_f_orders.filter(col(\"load_dttm\") > last_load_date)\n",
    "\n",
    "# Объединяем таблицы измерений и фактов в один df_joined\n",
    "df_joined = (\n",
    "    df_f_orders\n",
    "    .join(df_d_products, \"product_id\")\n",
    "    .join(df_d_craftsmans, \"craftsman_id\")\n",
    "    .join(df_d_customers, \"customer_id\")\n",
    ")\n",
    "\n",
    "# Добавляем \"report_period\" по дате создания заказа\n",
    "df_joined = df_joined.withColumn(\n",
    "    \"report_period\",\n",
    "    expr(\"date_format(order_created_date, 'yyyy-MM')\")\n",
    ")\n",
    "\n",
    "# Считаем длительность выполнения заказа (completion - created)\n",
    "df_joined = df_joined.withColumn(\n",
    "    \"days_to_complete\",\n",
    "    datediff(col(\"order_completion_date\"), col(\"order_created_date\"))\n",
    ")\n",
    "\n",
    "# Определяем приблизительный возраст клиента\n",
    "df_joined = df_joined.withColumn(\n",
    "    \"customer_age\",\n",
    "    datediff(current_date(), col(\"customer_birthday\")) / 365.25\n",
    ")\n",
    "\n",
    "# Агрегируем для витрины\n",
    "df_agg = df_joined.groupBy(\n",
    "    \"craftsman_id\",\n",
    "    \"craftsman_name\",\n",
    "    \"craftsman_address\",\n",
    "    \"craftsman_birthday\",\n",
    "    \"craftsman_email\",\n",
    "    \"report_period\"\n",
    ").agg(\n",
    "    spark_sum(col(\"product_price\") * 0.9).alias(\"craftsman_money\"),\n",
    "    spark_sum(col(\"product_price\") * 0.1).alias(\"platform_money\"),\n",
    "    count(\"order_id\").alias(\"count_order\"),\n",
    "    avg(\"product_price\").alias(\"avg_price_order\"),\n",
    "    avg(\"customer_age\").alias(\"avg_age_customer\"),\n",
    "    expr(\"percentile_approx(days_to_complete, 0.5)\").alias(\"median_time_order_completed\"),\n",
    "    count(when(col(\"order_status\") == \"created\", True)).alias(\"count_order_created\"),\n",
    "    count(when(col(\"order_status\") == \"in progress\", True)).alias(\"count_order_in_progress\"),\n",
    "    count(when(col(\"order_status\") == \"delivery\", True)).alias(\"count_order_delivery\"),\n",
    "    count(when(col(\"order_status\") == \"done\", True)).alias(\"count_order_done\"),\n",
    "    count(when(col(\"order_status\").isin(\"created\", \"in progress\", \"delivery\"), True)).alias(\"count_order_not_done\")\n",
    ")\n",
    "\n",
    "# Определяем самую популярную категорию товара для каждого мастера и периода\n",
    "df_product_category_count = df_joined.groupBy(\n",
    "    \"craftsman_id\",\n",
    "    \"report_period\",\n",
    "    \"product_type\"\n",
    ").agg(\n",
    "    count(\"product_id\").alias(\"product_count\")\n",
    ")\n",
    "\n",
    "# Задаём окно для сортировки product_type по убыванию product_count\n",
    "w_category = Window.partitionBy(\"craftsman_id\", \"report_period\").orderBy(col(\"product_count\").desc())\n",
    "# Находим категорию, которая идёт на 1-м месте (rank = 1)\n",
    "df_top_category = (\n",
    "    df_product_category_count\n",
    "    .withColumn(\"rank\", row_number().over(w_category))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .select(\n",
    "        \"craftsman_id\",\n",
    "        \"report_period\",\n",
    "        col(\"product_type\").alias(\"top_product_category\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Склеиваем итоговый датафрейм для витрины\n",
    "df_final_report = df_agg.join(df_top_category, [\"craftsman_id\", \"report_period\"], how=\"left\")\n",
    "\n",
    "# Записываем витрину в таблицу DWH\n",
    "df_final_report.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dwh.craftsman_report_datamart\",\n",
    "    mode=\"append\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Обновляем таблицу с датой последней загрузки,\n",
    "df_new_load_date = spark.range(1).select(current_date().alias(\"load_dttm\"))\n",
    "\n",
    "df_new_load_date.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dwh.load_dates_craftsman_report_datamart\",\n",
    "    mode=\"append\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
